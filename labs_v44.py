# -*- coding: utf-8 -*-
"""labs v34.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18QHJgOzjZ2j5hXtWKmyisx7OPEf8hQvJ
"""

###################################################################
# CVNN: COMPLEX-VALUED NEURAL NETWORK
###################################################################
# This is an identical copy of cvnn but it only uses ReLu in the three levels
# of possible different activation functions

import numpy as np

# SUMLISTARRAY sums two lists of arrays
def sumlistarray(A, B): # C
  sz = len(A)
  C = []
  for i in range(sz):
    levelsum = A[i] + B[i]
    C.append(levelsum)
  
  return C

# KLISTARRAY multiply a list of arrays by a scalar
def klistarray(k, A): # B
  sz = len(A)
  B = []
  for i in range(sz):
    levelkarr = k * A[i]
    B.append(levelkarr)
  
  return B

# MULTLISTARRAY sums two lists of arrays
def multlistarray(A, B): # C
  sz = len(A)
  C = []
  for i in range(sz):
    levelmult = np.real(A[i]) * np.real(B[i]) + 1j * np.imag(A[i]) * np.imag(B[i])
    C.append(levelmult)
  
  return C

# SQRTLISTARRAY square root of a list of arrays (piecewise if complex)
def sqrtlistarray(A): # B
  sz = len(A)
  B = []
  for i in range(sz):
    levelsqrt = np.sqrt(np.real(A[i])) + 1j * np.sqrt(np.imag(A[i]))
    B.append(levelsqrt)
  
  return B


# DIVLISTARRAY divide two lists of arrays piecewise
def divlistarray(A, B, epsilon): # C
  sz = len(A)
  C = []
  for i in range(sz):
    leveldiv = np.real(A[i])/(np.real(B[i]) + epsilon) + 1j * np.imag(A[i])/(np.imag(B[i]) + epsilon)
    C.append(leveldiv)
  
  return C

# RELISTARRAY real part of a list of arrays
#def relistarray(A): # B
#  sz = len(A)
#  B = []
#  for i in range(sz):
#    levelre = np.real(A[i])
#    B.append(levelre)
#  
#  return B

# MODCVNN2 returns the square module of a CVNN weight matrixes (norm2 sense)
def modcvnn2(CVNN): # mod2
  sz = len(CVNN)
  mod2 = 0
  for i in range(sz):
    mod2 += np.linalg.norm(CVNN[i], ord=2)**2

  return mod2

# VARREP Returns the cartesian product**dimn of set listn (recursive function)
def varrep(listn, dim):
  n = len(listn)
  if(dim == 1):
    Aret = np.zeros((n, 1), dtype=complex)
    for i in range(n):
      Aret[i][0] = listn[i]
  else:
    subrec = varrep(listn, dim - 1)
    sizsub = subrec.shape
    nsub = sizsub[0]
    msub = sizsub[1]

    for i in range(n):
      if i == 0:
        Aret = np.hstack((listn[i] * np.ones((nsub, 1), dtype=complex), subrec))
      else:
        Aret = np.vstack((Aret, np.hstack((listn[i] * np.ones((nsub, 1), dtype=complex), subrec))))
  return Aret 

# RANDMATRIX Returns random matrix of complex numbers of size (n,m)
# with real and imaginary parts between -1 and 1
# It returns real values only if realOnly = True
def randMatrix(n, m, realOnly):
  ret = 2 * (np.random.rand(n, m) - 0.5 * np.ones((n,m)))
  
  if(not realOnly):
    ret = ret + 2j * (np.random.rand(n, m) - 0.5 * np.ones((n,m)))
  
  return ret

# ININETWORK This function allocates and random initializes a complex-valued
# neural network (CVNN) weights matrixes based on the dimension 
# specifications passed in
# dimLine = [no. of inputs no. of nodes 1st layer no. of nodes 2nd layer
# ... no. of nodes output layer]
# It returns an array of arrays with the weight matrixes
# It returns real values only if realOnly = True
# It initializes biases to 0 if bias0 = True
# It initializes de first layer with integer combinations (eg. -2 1 -1 3)
# for a Fourier network implementation if fouser = True
def iniNetwork(dimLine, realOnly, bias0, fouser, Xmin = None, Xmax = None): # CVNN
  # dimLine basic checks
  sizd = dimLine.shape
  if(sizd[0] != 1):
    raise Exception('ERROR: dimLine must be a row vector')
    
  depth = sizd[1] # depth - 1 = Number of layers including input
  depthM1 = depth - 1
  
  if (np.sum(np.floor(dimLine) - np.absolute(dimLine)) != 0):
    raise Exception('ERROR: dimLine must be an array of positive integers')
  
  # Check the hypercube boundaries if passed 
  if(Xmin is not None and Xmax is not None):
    sizXmin = Xmin.shape
    xminm = sizXmin[0]
    xminn = sizXmin[1]
    if(xminn != 1):
      raise Exception('ERROR: Invalid input Xmin vector. It must be a column vector')
    sizXmax = Xmax.shape
    xmaxm = sizXmax[0]
    xmaxn = sizXmax[1]
    if(xmaxn != 1):
      raise Exception('ERROR: Invalid input Xmax vector. It must be a column vector')
    if(xmaxm != xminm):
      raise Exception('ERROR: Dimension mismatch of bounds vectors')

  # Return value allocation
  CVNN = []
  
  for l in range(depth - 1):
    if(not bias0):
      # Random bias
      CVNN.append(randMatrix(dimLine[0][l + 1], dimLine[0][l] + 1, realOnly))
    else:
      # All bias are initinialized to zero
      LthWeights = np.hstack((np.zeros((dimLine[0][l + 1], 1), dtype=complex), randMatrix(dimLine[0][l + 1], dimLine[0][l], realOnly)))
      CVNN.append(LthWeights)
    
  # If Fourier network ########### TMP nin = 1, 20 neurons in 1st layer
  if(fouser):
    FirstWeights = [-1/2, -4, -3, -2, -1, 0, 1, 2, 3, 4, 1/2, 1/2]
    Aret = varrep(FirstWeights, 1)
    CVNN[0][0:12,1:2] = Aret
    #CVNN[0][0:20,1:2] = Aret
    #CVNN[0][0:9,1:2] = Aret
    #  tryr = np.random.randint(0, 4)
    #  print(tryr) # 0, 1, 2, 3
  
  # If hypercubes boundaries are passed, distribute first hidden layer uniformly over a grid
  if(Xmin is not None and Xmax is not None): 
    # Dimension hardcoded, paralelepid ND case to be implemented
    # Line 1D case, paralelepid ND case to be implemented
    if(xmaxm == 1):
      Ngrid = dimLine[0][1]
      count = 0
      for x0 in np.linspace(Xmin[0][0], Xmax[0][0], Ngrid + 1):
        if(x0 < Xmax[0][0]):
          xgrid0 = x0 + (Xmax[0][0]-Xmin[0][0])/(2*Ngrid)
          CVNN[0][count][0] = -CVNN[0][count][1] * xgrid0 
          count += 1
    # Square 2D case, paralelepid ND case to be implemented
    if(xmaxm == 2):
      Ngrid = int(round(np.sqrt(dimLine[0][1])))
      count = 0
      for x0 in np.linspace(Xmin[0][0], Xmax[0][0], Ngrid + 1):
        for x1 in np.linspace(Xmin[1][0], Xmax[1][0], Ngrid + 1):
          if(x0 < Xmax[0][0] and x1 < Xmax[1][0]):
            xgrid0 = x0 + (Xmax[0][0]-Xmin[0][0])/(2*Ngrid)
            xgrid1 = x1 + (Xmax[1][0]-Xmin[1][0])/(2*Ngrid)
            CVNN[0][count][0] = -CVNN[0][count][1] * xgrid0 - CVNN[0][count][2] * xgrid1
            count += 1
     # Cubic 3D case, paralelepid ND case to be implemented
    if(xmaxm == 3):
      Ngrid = int(round(dimLine[0][1]**(1./3.)))
      count = 0
      for x0 in np.linspace(Xmin[0][0], Xmax[0][0], Ngrid + 1):
        for x1 in np.linspace(Xmin[1][0], Xmax[1][0], Ngrid + 1):
          for x2 in np.linspace(Xmin[2][0], Xmax[2][0], Ngrid + 1):
            if(x0 < Xmax[0][0] and x1 < Xmax[1][0] and x2 < Xmax[2][0]):
              xgrid0 = x0 + (Xmax[0][0]-Xmin[0][0])/(2*Ngrid)
              xgrid1 = x1 + (Xmax[1][0]-Xmin[1][0])/(2*Ngrid)
              xgrid2 = x2 + (Xmax[2][0]-Xmin[2][0])/(2*Ngrid)
              CVNN[0][count][0] = -CVNN[0][count][1] * xgrid0 - CVNN[0][count][2] * xgrid1 - CVNN[0][count][3] * xgrid2
              count += 1
              if (count == dimLine[0][1]):
                break;
          if (count == dimLine[0][1]):
            break;
        if (count == dimLine[0][1]):
          break;

  return CVNN


# RELU This function calculates the differentiable leaky ReLu 
# for np.array x and parameters a0, b0
def ReLu(x, a0, b0): # f
  sizx = x.shape
  nz = sizx[0]
  mz = sizx[1]
  
  # Return value allocation
  f = np.zeros((nz, mz))
  for n in range(nz):
    for m in range(mz):
      if(x[n][m] < 100): # Trick to avoid overflow of exp()
        f[n][m] = (a0 - b0) * np.log(1 + np.exp(x[n][m])) + b0 * x[n][m]
      else:
        f[n][m] = a0 * x[n][m]
        
  return f

# DRELU This function calculates the derivative of leaky ReLu 
# for np.array x and parameters a0, b0
def dReLu(x, a0, b0): # df
  sizx = x.shape
  nz = sizx[0]
  mz = sizx[1]
  
  # Return value allocation
  df = np.zeros((nz, mz))
  for n in range(nz):
    for m in range(mz):
      if(x[n][m] < 100): # Trick to avoid overflow of exp()
        df[n][m] = a0 - (a0 - b0) / (1 + np.exp(x[n][m]))
      else:
        df[n][m] = a0
        
  return df

# D2RELU This function calculates the derivative of leaky ReLu 
# for np.array x and parameters a0, b0
def d2ReLu(x, a0, b0): # d2f
  sizx = x.shape
  nz = sizx[0]
  mz = sizx[1]
  
  # Return value allocation
  d2f = np.zeros((nz, mz))
  for n in range(nz):
    for m in range(mz):
      if(x[n][m] < 100): # Trick to avoid overflow of exp()
        d2f[n][m] = (a0 - b0) * np.exp(x[n][m]) / (1 + np.exp(x[n][m]))**2
      else:
        d2f[n][m] = 0
        
  return d2f

# LSYMT This function calculates the differentiable leaky tanh 
# for np.array x and parameters a0, b0
def Lsymt(x, a0, b0): # f
  #sizx = x.shape
  #nz = sizx[0]
  #mz = sizx[1]
  
  ###### TEMPORARY
  bf = 2 / b0
  ls = 0.15 # leaky slope
    
  # Return value allocation
  #f = np.zeros((nz, mz))
  #for n in range(nz):
    #for m in range(mz):
      # Tested
      #f[n][m] = a0 * (ls * np.sqrt((bf ** 2 + x[n][m] ** 2)/(bf ** 2)) + 1 - ls) * np.tanh(b0 * x[n][m])
  f = a0 * (ls * np.sqrt((bf ** 2 + x ** 2)/(bf ** 2)) + 1 - ls) * np.tanh(b0 * x)

  return f

# DLSYMT This function calculates the derivative of leaky tanh 
# for np.array x and parameters a0, b0
def dLsymt(x, a0, b0): # df
  #sizx = x.shape
  #nz = sizx[0]
  #mz = sizx[1]

  ###### TEMPORARY
  bf = 2 / b0
  ls = 0.15 # leaky slope

  # Return value allocation
  #df = np.zeros((nz, mz))
  #for n in range(nz):
    #for m in range(mz):
      # Tested
      #df[n][m] = a0 * ((ls / (bf ** 2)) * np.power((bf ** 2 + x[n][m] ** 2)/(bf ** 2), -1/2) * x[n][m]) * np.tanh(b0 * x[n][m]) + a0 * b0 * (ls * np.sqrt((bf ** 2 + x[n][m] ** 2)/(bf ** 2)) + 1 - ls) * (1 - np.tanh(b0 * x[n][m]) ** 2)
  df = a0 * ((ls / (bf ** 2)) * np.power((bf ** 2 + x ** 2)/(bf ** 2), -1/2) * x) * np.tanh(b0 * x) + a0 * b0 * (ls * np.sqrt((bf ** 2 + x ** 2)/(bf ** 2)) + 1 - ls) * (1 - np.tanh(b0 * x) ** 2)      
  
  return df

# D2LSYMT This function calculates the derivative of leaky tanh
# for np.array x and parameters a0, b0
def d2Lsymt(x, a0, b0): # d2f
  sizx = x.shape
  nz = sizx[0]
  mz = sizx[1]

  raise Exception('ERROR: d2Lsymt NOT IMPLEMENTED YET !!! ')

  ###### TEMPORARY. REVIEW SECOND DERIVATIVE
  bf = 2 / b0
  ls = 0.15 # leaky slope

  # Return value allocation
  d2f = np.zeros((nz, mz))
  for n in range(nz):
    for m in range(mz):
      #if(x[n][m] < 100): # Trick to avoid overflow of exp()
      d2f[n][m] = a0 * (ls * np.sqrt((bf ** 2 + x[n][m] ** 2)/(bf ** 2)) + 1 - ls) * np.tanh(b0 * x[n][m])
      #else:
      #  f[n][m] = a0 * x[n][m]
        
  return d2f

# F Activation function (complex with complex argument z = x + i * y)
# f(z) = u(x,y) + i*v(x,y), z is a general complex np.array
# The activation function is level-dependant, distinguishing first hidden
# layer, any other hidden layer or output layer, with parameter values
# respectively level = 0, 1, 2
# a and b are 3-dimensional column vectors with function hyperparameters
# Its partial derivatives will be defined as 
# ux = du / dx; uy = du / dy; vx = dv / dx; vy = dv / dy;
# uxx = d2u / dx2; uxy = d2u / dxdy etc.
# It returns real values only if realOnly = True
# The type of activation function to be used is defined in actfun (list of length 3)
# actfun[0-2] can be "relu", "lsymt", "tanh", "gauss", "lin", "expi" # TO BE IMPLEMENTED? "sin", "cos"?
def f(z, level, a, b, realOnly, actfun): # fz
  # Input arguments validation
  if(level != 0 and level != 1 and level != 2):
    raise Exception('ERROR: Invalid level specification in activation function')
    
  siza = a.shape
  sizb = b.shape

  if(siza[0] != 3 or sizb[0] != 3 or siza[1] != 1 or sizb[1] != 1):
    raise Exception('ERROR: Invalid hyperparameter vectors. 3D column vectors expected')
  
  # Check activation function specifications for the three levels l = 0, 1, 2
  for l in range(3):
    if(actfun[l] != "relu" and actfun[l] != "lsymt" and actfun[l] != "tanh" and actfun[l] != "gauss" and actfun[l] != "lin" and actfun[l] != "expi" and actfun[l] != "asinh"):
      raise Exception('ERROR: Activation function specified layer unknown')

  x = np.real(z)
  y = np.imag(z)
  
  if(actfun[level] == "relu"):
    # Differentiable Leaky ReLu
    u = ReLu(x, a[level][0], b[level][0])
    v = ReLu(y, a[level][0], b[level][0])
  elif(actfun[level] == "lsymt"):
    # Differentiable Leaky symmetric tanh
    u = Lsymt(x, a[level][0], b[level][0])
    v = Lsymt(y, a[level][0], b[level][0])
  elif(actfun[level] == "tanh"):
    # Hyperbolic tangent
    u = a[level][0] * np.tanh(b[level][0] * x)
    v = a[level][0] * np.tanh(b[level][0] * y)
  elif(actfun[level] == "gauss"):
    # Square-integrable normalisation
    u = a[level][0] * x * np.exp(-b[level][0] * np.power(x, 2))
    v = a[level][0] * y * np.exp(-b[level][0] * np.power(y, 2))
  elif(actfun[level] == "lin"):
    # Linear activation function
    u = a[level][0] * x
    v = a[level][0] * y
  elif(actfun[level] == "expi"):
    # exp(ix) for Fourier series
    u = a[level][0] * np.cos(b[level][0] * x)
    v = a[level][0] * np.sin(b[level][0] * y)
  elif(actfun[level] == "asinh"):
      # arcsinh
      u = a[level][0] * np.arcsinh(b[level][0] * x)
      v = a[level][0] * np.arcsinh(b[level][0] * y)
      # Aborted test of fully complex activation function:
      #u = np.real(a[level][0] * np.arcsinh(b[level][0] * z))
      #v = np.imag(a[level][0] * np.arcsinh(b[level][0] * z))
  
  if(realOnly):
    v = 0 * v
    
  fz = u + 1j * v
  
  return fz 


# UX Partial derivative of the real part of the activation function f(z)
# with respect to x = real(z)
# See f
# It returns real values only if realOnly = True (by default)
def ux(z, level, a, b, realOnly, actfun): # uxret
  # Input arguments validation
  if(level != 0 and level != 1 and level != 2):
    raise Exception('ERROR: Invalid level specification in activation function')
    
  siza = a.shape
  sizb = b.shape
    
  if(siza[0] != 3 or sizb[0] != 3 or siza[1] != 1 or sizb[1] != 1):
    raise Exception('ERROR: Invalid hyperparameter vectors. 3D column vectors expected')

  # Check activation function specifications for the three levels l = 0, 1, 2
  for l in range(3):
    if(actfun[l] != "relu" and actfun[l] != "lsymt" and actfun[l] != "tanh" and actfun[l] != "gauss" and actfun[l] != "lin" and actfun[l] != "expi" and actfun[l] != "asinh"):
      raise Exception('ERROR: Activation function specified layer unknown')

  sizz = z.shape
  nz = sizz[0]
  mz = sizz[1]

  x = np.real(z)
  #y = np.imag(z)

  if(actfun[level] == "relu"):
    # Differentiable Leaky ReLu
    uxret = dReLu(x, a[level][0], b[level][0])
  elif(actfun[level] == "lsymt"):
    # Differentiable Leaky symmetric tanh
    uxret = dLsymt(x, a[level][0], b[level][0]) 
  elif(actfun[level] == "tanh"):
    # Hyperbolic tangent
    uxret = a[level][0] * b[level][0] * (np.ones((nz, mz)) - np.power(np.tanh(b[level][0] * x), 2))
  elif(actfun[level] == "gauss"):
    # Square-integrable normalisation
    uxret = a[level][0] * (np.ones((nz, mz)) - 2 * b[level][0] * np.power(x, 2)) * np.exp(-b[level][0] * np.power(x, 2))
  elif(actfun[level] == "lin"):
    # Linear activation function
    uxret = a[level][0] * np.ones((nz, mz))
  elif(actfun[level] == "expi"):
    # exp(ix) for Fourier series
    uxret = a[level][0] * b[level][0] * (-np.sin(b[level][0] * x))
  elif(actfun[level] == "asinh"):
    # arcsinh
    uxret = a[level][0] * b[level][0] / np.sqrt(1 + (b[level][0] * x) ** 2)
    # Aborted test of fully complex activation function:
    #uxret = np.real(a[level][0] * b[level][0] / np.sqrt(1 + (b[level][0] * z) ** 2))

  return uxret
  
  
# VY Partial derivative of the imaginary part of the activation function f(z)
# with respect to y = imag(z)
# See f
# It returns real values only if realOnly = True
def vy(z, level, a, b, realOnly, actfun): # vyret
  # Input arguments validation
  if(level != 0 and level != 1 and level != 2):
    raise Exception('ERROR: Invalid level specification in activation function')
    
  siza = a.shape
  sizb = b.shape
    
  if(siza[0] != 3 or sizb[0] != 3 or siza[1] != 1 or sizb[1] != 1):
    raise Exception('ERROR: Invalid hyperparameter vectors. 3D column vectors expected')

  # Check activation function specifications for the three levels l = 0, 1, 2
  for l in range(3):
    if(actfun[l] != "relu" and actfun[l] != "lsymt" and actfun[l] != "tanh" and actfun[l] != "gauss" and actfun[l] != "lin" and actfun[l] != "expi" and actfun[l] != "asinh"):
      raise Exception('ERROR: Activation function specified layer unknown')

  sizz = z.shape
  nz = sizz[0]
  mz = sizz[1]

  #x = np.real(z)
  y = np.imag(z)
    
  if(actfun[level] == "relu"):
    # Differentiable Leaky ReLu
    vyret = dReLu(y, a[level][0], b[level][0])
  elif(actfun[level] == "lsymt"):
    # Differentiable Leaky symmetric tanh
    vyret = dLsymt(y, a[level][0], b[level][0]) 
  elif(actfun[level] == "tanh"):
    # Hyperbolic tangent
    vyret = a[level][0] * b[level][0] * (np.ones((nz, mz)) - np.power(np.tanh(b[level][0] * y), 2))
  elif(actfun[level] == "gauss"):
    # Square-integrable normalisation
    vyret = a[level][0] * (np.ones((nz, mz)) - 2 * b[level][0] * np.power(y, 2)) * np.exp(-b[level][0] * np.power(y, 2))
  elif(actfun[level] == "lin"):
    # Linear activation function
    vyret = a[level][0] * np.ones((nz, mz))
  elif(actfun[level] == "expi"):
    # exp(ix) for Fourier series
    vyret = a[level][0] * b[level][0] * (np.cos(b[level][0] * y))
  elif(actfun[level] == "asinh"):
    # arcsinh
    vyret = a[level][0] * b[level][0] / np.sqrt(1 + (b[level][0] * y) ** 2)
    # Aborted test of fully complex activation function:
    #vyret = np.imag(a[level][0] * b[level][0] / np.sqrt(1 + (b[level][0] * z) ** 2))

  if(realOnly):
    vyret = 0 * vyret
    
  return vyret
        

# UXX 2nd partial derivative of the real part of the activation function f(z)
# with respect to x = real(z) twice
# See f
# It returns real values only if realOnly = True (by default)
def uxx(z, level, a, b, realOnly, actfun): # uxxret
  # Input arguments validation
  if(level != 0 and level != 1 and level != 2):
    raise Exception('ERROR: Invalid level specification in activation function');
    
  siza = a.shape
  sizb = b.shape
    
  if(siza[0] != 3 or sizb[0] != 3 or siza[1] != 1 or sizb[1] != 1):
    raise Exception('ERROR: Invalid hyperparameter vectors. 3D column vectors expected')

  # Check activation function specifications for the three levels l = 0, 1, 2
  for l in range(3):
    if(actfun[l] != "relu" and actfun[l] != "lsymt" and actfun[l] != "tanh" and actfun[l] != "gauss" and actfun[l] != "lin" and actfun[l] != "expi" and actfun[l] != "asinh"):
      raise Exception('ERROR: Activation function specified layer unknown')

  sizz = z.shape
  nz = sizz[0]
  mz = sizz[1]

  x = np.real(z)
  #y = np.imag(z)

  if(actfun[level] == "relu"):
    # Differentiable Leaky ReLu
    uxxret = d2ReLu(x, a[level][0], b[level][0])
  elif(actfun[level] == "lsymt"):
    # Differentiable Leaky symmetric tanh. WARNING second derivative of Lsymt not implemented so far
    uxxret = d2Lsymt(x, a[level][0], b[level][0]) 
  elif(actfun[level] == "tanh"):
    # Hyperbolic tangent
    uxxret = -2 * a[level][0] * (b[level][0]**2) * np.tanh(b[level][0] * x) * (np.ones((nz, mz)) - np.power(np.tanh(b[level][0] * x), 2))
  elif(actfun[level] == "gauss"):
    # Square-integrable normalisation
    uxxret = a[level][0] * 2 * b[level][0] * x * (2 * b[level][0] * np.power(x, 2) - 3 * np.ones((nz, mz))) * np.exp(-b[level][0] * np.power(x, 2))
  elif(actfun[level] == "lin"):
    # Linear activation function
    uxxret = np.zeros((nz, mz))
  elif(actfun[level] == "expi"):
    # exp(ix) for Fourier series
    uxxret = a[level][0] * (b[level][0]**2) * (-np.cos(b[level][0] * x))  
  elif(actfun[level] == "asinh"):
    # arcsinh
    uxxret = -a[level][0] * (b[level][0]**3) * x * np.power(1 + (b[level][0] * x) ** 2, -3/2)

  return uxxret    
    

# VYY 2nd partial derivative of the real part of the activation function f(z)
# with respect to x = real(z) twice
# See f
# It returns real values only if realOnly = True
def vyy(z, level, a, b, realOnly, actfun): # vyyret
  # Input arguments validation
  if(level != 0 and level != 1 and level != 2):
    raise Exception('ERROR: Invalid level specification in activation function');
    
  siza = a.shape
  sizb = b.shape
    
  if(siza[0] != 3 or sizb[0] != 3 or siza[1] != 1 or sizb[1] != 1):
    raise Exception('ERROR: Invalid hyperparameter vectors. 3D column vectors expected')

  # Check activation function specifications for the three levels l = 0, 1, 2
  for l in range(3):
    if(actfun[l] != "relu" and actfun[l] != "lsymt" and actfun[l] != "tanh" and actfun[l] != "gauss" and actfun[l] != "lin" and actfun[l] != "expi" and actfun[l] != "asinh"):
      raise Exception('ERROR: Activation function specified layer unknown')

  sizz = z.shape
  nz = sizz[0]
  mz = sizz[1]

  #x = np.real(z)
  y = np.imag(z)
    
  if(actfun[level] == "relu"):
    # Differentiable Leaky ReLu
    vyyret = d2ReLu(y, a[level][0], b[level][0])
  elif(actfun[level] == "lsymt"):
    # Differentiable Leaky symmetric tanh
    vyyret = d2Lsymt(y, a[level][0], b[level][0]) 
  elif(actfun[level] == "tanh"):
    # Hyperbolic tangent
    vyyret = -2 * a[level][0] * (b[level][0]**2) * np.tanh(b[level][0] * y) * (np.ones((nz, mz)) - np.power(np.tanh(b[level][0] * y), 2))
  elif(actfun[level] == "gauss"):
    # Square-integrable normalisation
    vyyret = a[level][0] * 2 * b[level][0] * y * (2 * b[level][0] * np.power(y, 2) - 3 * np.ones((nz, mz))) * np.exp(-b[level][0] * np.power(y, 2))
  elif(actfun[level] == "lin"):
    # Linear activation function
    vyyret = np.zeros((nz, mz))
  elif(actfun[level] == "expi"):
    # exp(ix) for Fourier series
    vyyret = a[level][0] * (b[level][0]**2) * (-np.sin(b[level][0] * y))  
  elif(actfun[level] == "asinh"):
    # arcsinh
    vyyret = -a[level][0] * (b[level][0]**3) * y * np.power(1 + (b[level][0] * y) ** 2, -3/2)

  if(realOnly):
    vyyret = 0 * vyyret;
    
  return vyyret        
    

# UY Partial derivative of the real part of the activation function f(z)
# with respect to y = imag(z)
# See f
# It returns real values only if realOnly = True (by default)
def uy(z, level, a, b, realOnly, actfun): # uyret
  # Check activation function specifications for the three levels l = 0, 1, 2
  for l in range(3):
    if(actfun[l] != "relu" and actfun[l] != "lsymt" and actfun[l] != "tanh" and actfun[l] != "gauss" and actfun[l] != "lin" and actfun[l] != "expi" and actfun[l] != "asinh"):
      raise Exception('ERROR: Activation function specified layer unknown')
  
  # No verifications. Always 0 for the current activation functions 
  sizz = z.shape
  nz = sizz[0]
  mz = sizz[1]

  #x = np.real(z)
  #y = np.imag(z)
    
  uyret = np.zeros((nz, mz))

  return uyret    
    
# UXY 2nd partial derivative of the real part of the activation function f(z)
# with respect to x = real(z) and y = imag(z)
# See f
# It returns real values only if realOnly = True (by default)
def uxy(z, level, a, b, realOnly, actfun): # uxyret
  # Check activation function specifications for the three levels l = 0, 1, 2
  for l in range(3):
    if(actfun[l] != "relu" and actfun[l] != "lsymt" and actfun[l] != "tanh" and actfun[l] != "gauss" and actfun[l] != "lin" and actfun[l] != "expi" and actfun[l] != "asinh"):
      raise Exception('ERROR: Activation function specified layer unknown')

  # No verifications. Always 0 for the current activation functions 
  sizz = z.shape
  nz = sizz[0]
  mz = sizz[1]

  #x = np.real(z)
  #y = np.imag(z)
    
  uxyret = np.zeros((nz, mz))

  return uxyret    
  
# UYY 2nd partial derivative of the real part of the activation function f(z)
# with respect to x = real(z) twice
# See f
# It returns real values only if realOnly = True (by default)
def uyy(z, level, a, b, realOnly, actfun): # uyyret
  # Check activation function specifications for the three levels l = 0, 1, 2
  for l in range(3):
    if(actfun[l] != "relu" and actfun[l] != "lsymt" and actfun[l] != "tanh" and actfun[l] != "gauss" and actfun[l] != "lin" and actfun[l] != "expi" and actfun[l] != "asinh"):
      raise Exception('ERROR: Activation function specified layer unknown')
  
  # No verifications. Always 0 for the current activation functions 
  sizz = z.shape
  nz = sizz[0]
  mz = sizz[1]

  #x = np.real(z)
  #y = np.imag(z)
    
  uyyret = np.zeros((nz, mz))

  return uyyret      
  
# VX Partial derivative of the imaginary part of the activation function f(z)
# with respect to x = real(z)
# See f
# It returns real values only if realOnly = True (by default)
def vx(z, level, a, b, realOnly, actfun): # vxret
  # Check activation function specifications for the three levels l = 0, 1, 2
  for l in range(3):
    if(actfun[l] != "relu" and actfun[l] != "lsymt" and actfun[l] != "tanh" and actfun[l] != "gauss" and actfun[l] != "lin" and actfun[l] != "expi" and actfun[l] != "asinh"):
      raise Exception('ERROR: Activation function specified layer unknown')
  
  # No verifications. Always 0 for the current activation functions 
  sizz = z.shape
  nz = sizz[0]
  mz = sizz[1]

  #x = np.real(z)
  #y = np.imag(z)
    
  vxret = np.zeros((nz, mz))

  return vxret    
    
# VXY 2nd partial derivative of the imaginary part of the activation function f(z)
# with respect to x = real(z) and y = imag(z)
# See f
# It returns real values only if realOnly = True (by default)
def vxy(z, level, a, b, realOnly, actfun): # vxyret
  # Check activation function specifications for the three levels l = 0, 1, 2
  for l in range(3):
    if(actfun[l] != "relu" and actfun[l] != "lsymt" and actfun[l] != "tanh" and actfun[l] != "gauss" and actfun[l] != "lin" and actfun[l] != "expi" and actfun[l] != "asinh"):
      raise Exception('ERROR: Activation function specified layer unknown')
  
  # No verifications. Always 0 for the current activation functions 
  sizz = z.shape
  nz = sizz[0]
  mz = sizz[1]

  #x = np.real(z)
  #y = np.imag(z)
    
  vxyret = np.zeros((nz, mz))

  return vxyret    
  
# VXX 2nd partial derivative of the imaginary part of the activation function f(z)
# with respect to x = real(z) twice
# See f
# It returns real values only if realOnly = True (by default)
def vxx(z, level, a, b, realOnly, actfun): # vxxret
  # Check activation function specifications for the three levels l = 0, 1, 2
  for l in range(3):
    if(actfun[l] != "relu" and actfun[l] != "lsymt" and actfun[l] != "tanh" and actfun[l] != "gauss" and actfun[l] != "lin" and actfun[l] != "expi" and actfun[l] != "asinh"):
      raise Exception('ERROR: Activation function specified layer unknown')

  # No verifications. Always 0 for the current activation functions 
  sizz = z.shape
  nz = sizz[0]
  mz = sizz[1]

  #x = np.real(z)
  #y = np.imag(z)
    
  vxxret = np.zeros((nz, mz))

  return vxxret        

# GAUSSENV defines n-dimensional gaussian envelope with bounds Xmin and Xmax
# Standard deviantion of the nth diemnsion is (Xmax(n) - Xmin(n)) / 4
# The envelope is applied to an arbitrary ANN output to make it square-integrable
# cusp = True includes a cusp at the origin multiplying by term exp(-r/(1 + br))
def gaussenv(X, Xmin = None, Xmax = None, cusp = None):  
  a = 1 # CHAPUCILLA cusp's parameter
  b = 2 # CHAPUCILLA cusp's parameter
  if(Xmin is not None and Xmax is not None and cusp is not None):
    sizX = X.shape
    xm = sizX[0]
    xn = sizX[1]
    if(xn != 1):
      raise Exception('ERROR: Invalid input X vector. It must be a column vector')
    sizXmin = Xmin.shape
    xminm = sizXmin[0]
    xminn = sizXmin[1]
    if(xminn != 1):
      raise Exception('ERROR: Invalid input Xmin vector. It must be a column vector')
    sizXmax = Xmax.shape
    xmaxm = sizXmax[0]
    xmaxn = sizXmax[1]
    if(xmaxn != 1):
      raise Exception('ERROR: Invalid input Xmax vector. It must be a column vector')
    if(xmaxm != xm or xminm != xm):
      raise Exception('ERROR: Dimension mismatch of bounds vectors')
    # Variance vector calculation
    S2m1 = 1/(np.real((Xmax - Xmin)/4))**2
    # Input values must be real
    X = np.real(X)
    # Return value is the gaussian envelope
    if(cusp == False):
      env = np.exp(np.sum(-X*X*S2m1)/2)
    elif(cusp == True):
      r = np.linalg.norm(X, ord=2)
      env = np.exp(np.sum(-X*X*S2m1)/2) * np.exp(-a * r / (1 + b * r))
  else: # gaussenv(X) = 1 by default
    env = 1
  
  return env

# DGAUSSENV gradient of GAUSSENV with respect to the (real) inputs X
# Returns a 1-row vector
# cusp = True includes a cusp at the origin multiplying by term exp(-r/(1 + br))
def dgaussenv(X, Xmin, Xmax, cusp):  
  a = 1 # CHAPUCILLA cusp's parameter
  b = 2 # CHAPUCILLA cusp's parameter
  # Size checks
  sizX = X.shape
  xm = sizX[0]
  xn = sizX[1]
  if(xn != 1):
    raise Exception('ERROR: Invalid input X vector. It must be a column vector')
  sizXmin = Xmin.shape
  xminm = sizXmin[0]
  xminn = sizXmin[1]
  if(xminn != 1):
    raise Exception('ERROR: Invalid input Xmin vector. It must be a column vector')
  sizXmax = Xmax.shape
  xmaxm = sizXmax[0]
  xmaxn = sizXmax[1]
  if(xmaxn != 1):
    raise Exception('ERROR: Invalid input Xmax vector. It must be a column vector')
  if(xmaxm != xm or xminm != xm):
    raise Exception('ERROR: Dimension mismatch of bounds vectors')
  # Variance vector calculation
  S2m1 = 1/(np.real((Xmax - Xmin)/4))**2
  # Input values must be real
  X = np.real(X)
  # Gradient of the envelope is -the transposed input * 1/s2 * gaussian envelope
  denv = -np.transpose(X) * np.transpose(S2m1) * np.exp(np.sum(-X*X*S2m1)/2)
  dgenv = np.zeros((1, xm))
  if(cusp == False):
    dgenv = denv
  elif(cusp == True):
    r = np.linalg.norm(X, ord=2)
    # Gradient of cusp
    dcusp = (-a * np.transpose(X) / (r * (1 + b  *r)**2)) * np.exp(-a * r / (1 + b * r))
    # Gradient of env * cusp
    dgenv = denv * np.exp(-a * r / (1 + b * r)) + dcusp * np.exp(np.sum(-X*X*S2m1)/2)

  return dgenv

# D2GAUSSENV second derivative of GAUSSENV with respect to the (real) inputs X
# Returns a 1-row vector
# cusp = True includes a cusp at the origin multiplying by term exp(-r/(1 + br))
def d2gaussenv(X, Xmin, Xmax, cusp):  
  a = 1 # CHAPUCILLA cusp's parameter
  b = 2 # CHAPUCILLA cusp's parameter
  # Size checks
  sizX = X.shape
  xm = sizX[0]
  xn = sizX[1]
  if(xn != 1):
    raise Exception('ERROR: Invalid input X vector. It must be a column vector')
  sizXmin = Xmin.shape
  xminm = sizXmin[0]
  xminn = sizXmin[1]
  if(xminn != 1):
    raise Exception('ERROR: Invalid input Xmin vector. It must be a column vector')
  sizXmax = Xmax.shape
  xmaxm = sizXmax[0]
  xmaxn = sizXmax[1]
  if(xmaxn != 1):
    raise Exception('ERROR: Invalid input Xmax vector. It must be a column vector')
  if(xmaxm != xm or xminm != xm):
    raise Exception('ERROR: Dimension mismatch of bounds vectors')
  # Variance vector calculation
  S2m1 = 1/(np.real((Xmax - Xmin)/4))**2
  # Input values must be real
  X = np.real(X)
  d2genv = np.zeros((1, xm))
  # Second derivative value is -the transposed input * 1/s2 * gaussian envelope
  d2env = (np.transpose(X)**2 * np.transpose(S2m1)**2 - np.transpose(S2m1)) * np.exp(np.sum(-X*X*S2m1)/2)
  if(cusp == False):
    d2genv = d2env
  elif(cusp == True):
    r = np.linalg.norm(X, ord=2)
    denv = -np.transpose(X) * np.transpose(S2m1) * np.exp(np.sum(-X*X*S2m1)/2)
    dcusp = (-a * np.transpose(X) / (r*(1 + b*r)**2)) * np.exp(-a * r / (1 + b * r))
    d2cusp = dcusp * (-a * np.transpose(X) / (r*(1 + b*r)**2)) + np.exp(-a * r / (1 + b * r)) * (-a * np.ones((1, xm)) / (r*(1 + b*r)**2) + a * np.transpose(X)**2 * (1 + 3*b*r) / (r**3 * (1 + b*r)**3))
    # Second derivatives to be returned
    d2genv = d2env * np.exp(-a * r / (1 + b * r)) + 2 * denv * dcusp + d2cusp * np.exp(np.sum(-X*X*S2m1)/2)
  
  return d2genv


# FEEDFWD Feed perceptron forward. 
# Arguments: Xin, input signals to the network
#            CVNN weight matrices organized in a cell depth - 1 long
#            a, b: activation function hyperparameters
#            Xmin, Xmax optional bounds if the wave function is forced to be square-integrable (multiplied bya gaussian envelope, see GAUSSENV)
#            in which case the output of the "standard" perceptron is multiplied by exp(-x^2/2std^2)
# Returns: OC cell of vectors with the output of each layer
#          ZC net value out of each layer (argument of the activation
#          function)
# It operates with real values only if realOnly = True
def feedFwd(Xin, CVNN, a, b, realOnly, actfun, Xmin = None, Xmax = None, cusp = None): # ZC, OC
  if((Xmin is not None and Xmax is None) or (Xmin is None and Xmax is not None)):
    raise Exception('ERROR: Internal error. Optional arguments Xmin, Ymin must be both present or missing')
  # Sizes and size checks
  depthM1 = len(CVNN);
  # Depth of the perceptron counting the input layer
  depth = depthM1 + 1
  sizxin = Xin.shape
  nin = sizxin[0]
  min = sizxin[1]
  if(min != 1):
    raise Exception('ERROR: Invalid input vector. It must be a column vector')

  # CVNN cascade sizes check
  priorInputs = nin
  for l in range(depthM1):
    sizcvnn = CVNN[l].shape
    nw = sizcvnn[0]
    mw = sizcvnn[1]
    if(mw != priorInputs + 1):
      raise Exception('ERROR: Invalid weight matrix in cell CVNN (dimension mismatch)')
    priorInputs = nw
  
  # Return cells allocation
  ZC = []
  OC = []
  
  # First hidden layer outputs
  oneXin = np.vstack((np.ones((1,1)), Xin)) # Fixed input to 1 is added (bias)
  ZC.append(np.matmul(CVNN[0], oneXin))
  OC.append(f(ZC[0], 0, a, b, realOnly, actfun)) # Level 0 activation function
  
  # Propagation up to the output layer
  for l in range(1, depthM1):
    oneOin = np.vstack((np.ones((1,1)), OC[l - 1])) # Fixed input to 1 is added (bias)
    ZC.append(np.matmul(CVNN[l], oneOin))
    if(l < depthM1 - 1):
      OC.append(f(ZC[l], 1, a, b, realOnly, actfun)) # Intermmidiate level activation function
    else:
      OC.append(f(ZC[l], 2, a, b, realOnly, actfun) * gaussenv(Xin, Xmin, Xmax, cusp)) # Output activation function
    
  return ZC, OC


# FEEDBWGRTADIENT Feed perceptron backwards with a single training sample. 
# Arguments: Xin, input sample to the network
#            Dout, expected outputs of the network for the given inputs (not mandatory)
#            Current CVNN weight matrices organized in a list depth - 1 long
#            a, b: activation function hyperparameters
#            actfun: type of activation function per layer level (0, 1, 2)
#            Xmin, Xmax optional bounds if the wave function is forced to be square-integrable (multiplied bya gaussian envelope, see GAUSSENV)
# Returns: dW : list with the -gradient respect to the weights (list of matrices)
# It operates with real values only if realOnly = True
# mode = "bpstd", "bpfirst0", "bpfirstim0", "gradfiR", "gradfiI"
# "bpstd" -> standard complex backpropagation 
# "bpfirst0" -> All layers are calculated and first hidden layer forced to 0
# "bpfirstim0" -> forces imaginary part of weights in first hidden layer to 0
# "gradfiR" -> ignores Dout and injects delta = 1 for gradient of Re(output)
# "gradfii" -> ignores Dout and injects delta = 1j for gradient of Im(output)
def feedBwdGradient(Xin, Dout, CVNN, a, b, realOnly, actfun, mode, Xmin = None, Xmax = None, cusp = None): # dW
  # Internal grammatical check
  if(mode != "bpstd" and mode != "bpfirst0" and mode != "bpfirstim0" and mode != "gradfiR" and mode != "gradfiI"):
    raise Exception('ERROR: Invalid calculation mode specification')
  # Sizes and size checks
  depthM1 = len(CVNN);
  # Depth of the perceptron counting the input layer
  depth = depthM1 + 1
  sizxin = Xin.shape
  nin = sizxin[0]
  min = sizxin[1]
  if(min != 1):
    raise Exception('ERROR: Invalid input vector. It must be a column vector')

  # Intermidiate and return values allocation
  delta = []
  dW = []  
    
  # CVNN cascade sizes check
  priorInputs = nin
  for l in range(depthM1):
    sizcvnn = CVNN[l].shape
    nw = sizcvnn[0]
    mw = sizcvnn[1]
    if(mw != priorInputs + 1):
      raise Exception('ERROR: Invalid weight matrix in cell CVNN (dimension mismatch)')
    # Allocation
    dW.append(np.zeros((nw, mw)))
    priorInputs = nw
    
  sizdout = Dout.shape;
  nout = sizdout[0]
  mout = sizdout[1]
  if(mout != 1):
    raise Exception('ERROR: Invalid expected output vector. It must be a column vector')
  if(nout != priorInputs):
    raise Exception('ERROR: Invalid expected output vector. Dimension mismatch')
  
  # Feed forward
  #ZC, OC = feedFwd(Xin, CVNN, a, b, realOnly, actfun)
  ZC, OC = feedFwd(Xin, CVNN, a, b, realOnly, actfun, Xmin, Xmax, cusp)
  
  # delta vectors allocation
  for l in range(depthM1):
    delta.append(0 * ZC[l])
    
  # Backpropagation: delta calculation
  # Cascade down from top for all modes excluding pure gradients of the output
  if(mode == "bpstd" or mode == "bpfirst0" or mode == "bpfirstim0"):
    delta[depthM1 - 1] = (Dout - OC[depthM1 - 1]) * gaussenv(Xin, Xmin, Xmax, cusp)
  elif(mode == "gradfiR"):
    # pure gradient of Re(output), ignores Dout and injects delta = 1 for gradient of output calculation with respect to the weights
    delta[depthM1 - 1] = np.ones((nout, 1), dtype=complex) * gaussenv(Xin, Xmin, Xmax, cusp)
  elif(mode == "gradfiI"):
    # pure gradient of Im(output), ignores Dout and injects delta = 1j for gradient of output calculation with respect to the weights
    delta[depthM1 - 1] = (1j) * np.ones((nout, 1), dtype=complex) * gaussenv(Xin, Xmin, Xmax, cusp)

  for r in range ((depthM1 - 2), -1, -1):
    level = -1
    # Level type calculation for activation function choice
    if((r + 1) == 0):
      level = 0 # Code never passes through here
    elif((r + 1) < (depthM1 - 1)):
      level = 1
    elif((r + 1) == (depthM1 - 1)):
      level = 2   
    # D: auxiliary column vector indexed by k for layer r+1    
    # General act function case is inactive
    #D = (ux(ZC[r+1], level, a, b, realOnly, actfun) + 1j * uy(ZC[r+1], level, a, b, realOnly, actfun)) * np.real(delta[r+1]) + (vx(ZC[r+1], level, a, b, realOnly, actfun) + 1j * vy(ZC[r+1], level, a, b, realOnly, actfun)) * np.imag(delta[r+1])
    # Pure split-type act function case (activated)
    D = (ux(ZC[r+1], level, a, b, realOnly, actfun)) * np.real(delta[r+1]) + (1j * vy(ZC[r+1], level, a, b, realOnly, actfun)) * np.imag(delta[r+1])
    sizcvnnrP1 = CVNN[r+1].shape
    mw1 = sizcvnnrP1[1]
    delta[r] = np.matmul(np.transpose(np.conj(CVNN[r+1][:,1:mw1])), D)   
    
  # Gradient (minus) for Weights update
  # First hidden layer
  # All modes
  # General act function case is inactive
  #D = (ux(ZC[0], 0, a, b, realOnly, actfun) + 1j * uy(ZC[0], 0, a, b, realOnly, actfun)) * np.real(delta[0]) + (vx(ZC[0], 0, a, b, realOnly, actfun) + 1j * vy(ZC[0], 0, a, b, realOnly, actfun)) * np.imag(delta[0])
  # Pure split-type act function case (activated)
  D = (ux(ZC[0], 0, a, b, realOnly, actfun)) * np.real(delta[0]) + (1j * vy(ZC[0], 0, a, b, realOnly, actfun)) * np.imag(delta[0])
  dW[0] = np.transpose(np.matmul(np.conj(np.vstack((np.ones((1, 1)), Xin))), np.transpose(D)))
  if(mode == "bpfirst0"):
    # First layer is not trained, truncate
    dW[0] = 0 * dW[0]
  if(mode == "bpfirstim0"):
    # First layer has no imaginary parts
    dW[0] = np.real(dW[0])
  # Second hidden layer up to output
  for r in range (1, depthM1):
    level = -1
    # Level type calculation for activation function choice
    if(r == 0):
      level = 0 # Code never passes through here
    elif(r < (depthM1 - 1)):
      level = 1
    elif(r == (depthM1 - 1)):
      level = 2
    D = (ux(ZC[r], level, a, b, realOnly, actfun) + 1j * uy(ZC[r], level, a, b, realOnly, actfun)) * np.real(delta[r]) + (vx(ZC[r], level, a, b, realOnly, actfun) + 1j * vy(ZC[r], level, a, b, realOnly, actfun)) * np.imag(delta[r])
    dW[r] = np.transpose(np.matmul(np.conj(np.vstack((np.ones((1,1)), OC[r-1]))), np.transpose(D)))

  return dW
             
# FEEDBWDBATCH Feed perceptron backwards with a batch of training sample. 
# Arguments: tXin, input samples to the network (#batch of them)
#            tDout, expected outputs of the network for the given inputs (#batch of them)
#            Current CVNN weight matrices organized in a list depth - 1 long
#            a, b: activation function hyperparameters
#            Xmin, Xmax optional bounds if the wave function is forced to be square-integrable (multiplied bya gaussian envelope, see GAUSSENV)
# Returns: CVNN : adjusted network parameters
# It operates with real values only if realOnly = True
# First hidden layer options specified with 'mode' (see FEEDBWDGRADIENT)
def feedBwdBatch(tXin, tDout, CVNN, a, b, realOnly, actfun, mode, Xmin = None, Xmax = None, cusp = None): # dW
  # Sizes and size checks
  depthM1 = len(CVNN);
  # Depth of the perceptron counting the input layer
  depth = depthM1 + 1
  # tXin and tDout containt #batch samples piled up vertically 
  # Basic size checks and batch size retrieval
  siztxin = tXin.shape
  batch = siztxin[0]
  nin = siztxin[1] # number of inputs
  siztdout = tDout.shape
  batch1 = siztdout[0]
  nout = siztdout[1] # number of outputs
  if(batch != batch1):
    raise Exception('ERROR: Inconsistent batch size between input samples and expected values')
  # dW list of matrices allocation (they will contain the gradient and thus the increments to CVNN)
  dW = []
  for l in range(depthM1):
    dW.append(CVNN[l] * 0)
  
  # Batch of gradients retrieval and accumulated calculation (times alfa, learning rate)
  for bat in range(batch):
    dWinc = feedBwdGradient(np.transpose(tXin[bat, :] * np.ones((1, nin))), np.transpose(tDout[bat, :] * np.ones((1, nout))), CVNN, a, b, realOnly, actfun, mode, Xmin, Xmax, cusp)
    # dW increment
    for l in range(depthM1):
      dW[l] = dW[l] + dWinc[l]
  
  return dW # Returns gradient "vector" (matrices)


# Total error for training data tD and expected output yD
# It operates with real values only if realOnly = True
def totalError(tXin, tDout, CVNN, a, b, realOnly, actfun, Xmin = None, Xmax = None, cusp = None): # e, testcurve
  # Sizes and size checks
  depthM1 = len(CVNN);
  # Depth of the perceptron counting the input layer
  depth = depthM1 + 1
  # tXin and tDout containt all samples samples piled up vertically 
  # Basic size checks and batch consistency
  siztxin = tXin.shape
  N = siztxin[0]
  nin = siztxin[1] # number of inputs
  siztdout = tDout.shape
  N1 = siztdout[0]
  nout = siztdout[1] # number of outputs
  if(N != N1):
    raise Exception('ERROR: Inconsistent size between input samples and expected values')
  
  # Allocate
  testcurve = np.zeros((N, nout), dtype=complex)
  e = 0
    
  for s in range(N):
    #print(np.transpose(tXin[s, :] * np.ones((1, nin))))
    ZC, OC = feedFwd(np.transpose(tXin[s, :] * np.ones((1, nin))), CVNN, a, b, realOnly, actfun, Xmin, Xmax, cusp)
    #ZC, OC = feedFwd(Xin, CVNN, a, b, realOnly, actfun)
    #print("NO PETAS AQUI")
    testcurve[s, :] = np.transpose(OC[depthM1 - 1])
    currentErr = np.transpose(tDout[s, :] - testcurve[s, :])
    e = e + np.sqrt(np.matmul(np.transpose(np.conj(currentErr)), currentErr))
   
  e = e / N
    
  #print('Current error:', e)

  return e, testcurve


# FEEDBWDALL Feed perceptron backwards batch by batch for the whole
#            dataset and nepochs epochs. 
# Arguments: tXin, input samples to the network 
#            tDout, expected outputs of the network for the given inputs 
#            Initial CVNN weight matrices organized in a list depth - 1 long
#            a, b: activation function hyperparameters
#            alfa: learning rate
#            batch: batch size
#            nepochs: number of epochs
#            Xmin, Xmax optional bounds if the wave function is forced to be square-integrable (multiplied bya gaussian envelope, see GAUSSENV)
# Returns: CVNN : adjusted network parameters
# It operates with real values only if realOnly = True
# First hidden layer options specified with 'mode' (see FEEDBWDGRADIENT)
def feedBwdAll(tXin, tDout, CVNN, a, b, realOnly, actfun, mode, alfa, beta1, beta2, epsilon, batch, nepochs, Xmin = None, Xmax = None, cusp = None, filename = None): # CVNN, e, testcurve
  # Sizes and size checks
  depthM1 = len(CVNN);
  # Depth of the perceptron counting the input layer
  depth = depthM1 + 1
  # tXin and tDout containt all samples samples piled up vertically 
  # Basic size checks and batch consistency
  siztxin = tXin.shape
  N = siztxin[0]
  nin = siztxin[1] # number of inputs
  siztdout = tDout.shape
  N1 = siztdout[0]
  nout = siztdout[1] # number of outputs
  if(N != N1):
    raise Exception('ERROR: Inconsistent size between input samples and expected values')
  if(batch > N):
    raise Exception('ERROR: Batch must be smaller than the dataset size')
  if(batch < 1 or np.floor(batch) != batch):
    raise Exception('ERROR: batch must be a positive integer')
  
  # Calculate number of batches nbatch. The last one may be smaller than #batch
  nbatch = np.floor(N / batch)
  nbatch = nbatch.astype(int)
  
  # Open results file if specified in the arguments
  if(filename is not None):
    fileo = open(filename,"w+")

  # Auxiliary vectors allocation for Adam implementation
  #mt = []
  #vt = []
  #for l in range(depthM1):
  #  mt.append(CVNN[l] * 0)
  #  vt.append(CVNN[l] * 0)
  mt = klistarray(0, CVNN)
  vt = klistarray(0, CVNN)
  # Adam time step
  t = 0
  # Feed backwards loop
  for ep in range(nepochs):
    # Generate random sequence to batch the dataset
    randseq = np.random.permutation(N)
    for bat in range(nbatch):
      t = t + 1
      dW = feedBwdBatch(tXin[randseq[bat * batch : (bat + 1) * batch], :], tDout[randseq[bat * batch : (bat + 1) * batch], :], CVNN, a, b, realOnly, actfun, mode, Xmin, Xmax, cusp)
      # Error print
      #print('Epoch:', ep, ' batch:', bat)  
      # CVNN update
      #for l in range(depthM1):
        # Adam algorithm
        #mt[l] = beta1 * mt[l] + (1 - beta1) * dW[l]
        #vt[l] = beta2 * vt[l] + (1 - beta2) * (dW[l] * dW[l])
        #CVNN[l] = CVNN[l] + alfa * ((1 / (1 - np.power(beta1, t))) * mt[l]) / (np.sqrt((1 / (1 - np.power(beta2, t))) * vt[l]) + epsilon)
        # Standard back propagation (back up)
        #CVNN[l] = CVNN[l] + alfa * dW[l]     
      #e, testcurve = totalError(tXin, tDout, CVNN, a, b)
      # Adam implementation
      mt = sumlistarray(klistarray(beta1, mt), klistarray(1 - beta1, dW))
      vt = sumlistarray(klistarray(beta2, vt), klistarray(1 - beta2, multlistarray(dW, dW)))
      stepW = divlistarray(klistarray(alfa / (1 - np.power(beta1, t)), mt), sqrtlistarray(klistarray(1 / (1 - np.power(beta2, t)), vt)), epsilon)
      CVNN = sumlistarray(CVNN, stepW)
    # Check if an 'odd' last batch is needed
    if(nbatch < (N / batch)):
      t = t + 1
      dW = feedBwdBatch(tXin[randseq[nbatch * batch : N], :], tDout[randseq[nbatch * batch : N], :], CVNN, a, b, realOnly, actfun, mode, Xmin, Xmax, cusp)
      # Error print
      #print('Epoch:', ep, ' batch:', batch)
      # CVNN update
      #for l in range(depthM1):
        # Adam algorithm
        #mt[l] = beta1 * mt[l] + (1 - beta1) * dW[l]
        #vt[l] = beta2 * vt[l] + (1 - beta2) * (dW[l] * dW[l])
        #CVNN[l] = CVNN[l] + alfa * ((1 / (1 - np.power(beta1, t))) * mt[l]) / (np.sqrt ((1 / (1 - np.power(beta2, t))) * vt[l]) + epsilon)
        # Standard back propagation (back up)
        #CVNN[l] = CVNN[l] + alfa * dW[l] 
      #e, testcurve = totalError(tXin, tDout, CVNN, a, b)
      # Adam implementation
      mt = sumlistarray(klistarray(beta1, mt), klistarray(1 - beta1, dW))
      vt = sumlistarray(klistarray(beta2, vt), klistarray(1 - beta2, multlistarray(dW, dW)))
      stepW = divlistarray(klistarray(alfa / (1 - np.power(beta1, t)), mt), sqrtlistarray(klistarray(1 / (1 - np.power(beta2, t)), vt)), epsilon)
      CVNN = sumlistarray(CVNN, stepW)

    # Write error to file if specified in the arguments
    if(filename is not None):
      e, testcurve = totalError(tXin, tDout, CVNN, a, b, realOnly, actfun, Xmin, Xmax, cusp)
      fileo.write("%d %f\r\n" % (ep + 1, np.real(e)))

  e, testcurve = totalError(tXin, tDout, CVNN, a, b, realOnly, actfun, Xmin, Xmax, cusp)
  if(filename is not None):
    fileo.close()
  return CVNN, e, testcurve

# DPROPLAYER Propagates the 1st and 2nd derivative through the CVNN
# on a layer to layer basis
# with respect to the (real) input variables
# It takes previous level arguments (P) as well as the number of inputs (nin)
# and intermidiate level outputs (OC) and nets (ZC)
# and network weights (CVNN) and calculates next level (N, 
# corresponding to the inputs of r+1)
# OX are complex-valued matrices with as many rows as neurons in the
# level and as many columns as number of inputs
# OX stands for derivative (1st or 2nd) of layer output with repect to Xin
# It operates with real values only if realOnly = True
def dPropLayer(nin, ZC, OC, OXP1, OXP2, CVNN, r, a, b, realOnly, actfun): # OXN1, OXN2    # GAUSSENV IS ONLY APPLIED IN FUNCTION DPROPALL
  # Sizes and size checks
  depthM1 = len(CVNN)
  # Depth of the perceptron counting the input layer
  depth = depthM1 + 1
  if(r != np.floor(r) or r < 0 or r > (depthM1 - 1)):
    raise Exception('ERROR: Invalid level number r')
  
  # Derivative matrixes (of the previous level) dimension check
  sizoxp1 = OXP1.shape
  noxp1 = sizoxp1[0]
  moxp1 = sizoxp1[1]
  
  sizoxp2 = OXP2.shape
  noxp2 = sizoxp2[0]
  moxp2 = sizoxp2[1]
  
  sizlayer = CVNN[r].shape
  inpsneur = sizlayer[1]
  
  # OXP1 and OXP2 have as many columns as network inputs (partial derivative matrix)
  # and as many rows as inputs of the layer - 1
  if(moxp1 != nin or moxp2 != nin or noxp1 != (inpsneur - 1) or noxp2  != (inpsneur -1)):
    raise Exception('ERROR: Invalid dimensions for derivative matrices (inputs)')
   
  level = -1
  # Level type calculation for activation function choice
  if(r == 0):
    level = 0
  elif(r < (depthM1 - 1)):
    level = 1
  elif(r == (depthM1 - 1)):
    level = 2

  # Return matrices allocation
  #nneurn = sizlayer[0] 
  #OXN1 = np.zeros((nneurn, nin))
  #OXN2 = np.zeros((nneurn, nin))
    
  # Temporary derivative matrices XXN1, YXN1 allocation
  #XXN1 = np.zeros((nneurn, nin))
  #YXN1 = np.zeros((nneurn, nin))
    
  # Temporary derivatives calculation: 
  # A row of zeroes is added to the header of the input matrices for convenience
  OXP1 = np.vstack((np.zeros((1, nin)), OXP1))
  OXP2 = np.vstack((np.zeros((1, nin)), OXP2))
 
  # Calculations used more than once:
  # The general case act function is commented, only pure split-type activated
  # For 1st and 2nd derivatives
  XXN1 = np.real(np.matmul(CVNN[r], OXP1))
  YXN1 = np.imag(np.matmul(CVNN[r], OXP1))
  # For second derivative
  XXN2 = np.real(np.matmul(CVNN[r], OXP2))
  YXN2 = np.imag(np.matmul(CVNN[r], OXP2))
  # Partial derivatives column vectors
  UXJ = ux(ZC[r], level, a, b, realOnly, actfun)
  #UYJ = uy(ZC[r], level, a, b, realOnly, actfun)
  #VXJ = vx(ZC[r], level, a, b, realOnly, actfun)
  VYJ = vy(ZC[r], level, a, b, realOnly, actfun)
  UXXJ = uxx(ZC[r], level, a, b, realOnly, actfun)
  #UYYJ = uyy(ZC[r], level, a, b, realOnly, actfun)
  #UXYJ = uxy(ZC[r], level, a, b, realOnly, actfun)
  #VXXJ = vxx(ZC[r], level, a, b, realOnly, actfun)
  VYYJ = vyy(ZC[r], level, a, b, realOnly, actfun)
  #VXYJ = vxy(ZC[r], level, a, b, realOnly, actfun)
  
  # Return matrices calculation
  #OXN1 = (UXJ * XXN1 + UYJ * YXN1) + 1j * (VXJ * XXN1 + VYJ * YXN1)
  OXN1 = (UXJ * XXN1) + 1j * (VYJ * YXN1)
  # OXN2 real part
  #OXN2 = (UXXJ * np.power(XXN1, 2) + UYYJ * np.power(YXN1, 2) + 2 * UXYJ * (XXN1 * YXN1)) + (UXJ * XXN2 + UYJ * YXN2)
  OXN2 = (UXXJ * np.power(XXN1, 2)) + (UXJ * XXN2)
  # OXN2 imaginary part
  OXN2 = OXN2 + 1j * (VYYJ * np.power(YXN1, 2)) + 1j * (VYJ * YXN2)
  
  return OXN1, OXN2


# DPROPALL Propagates the first and second derivatives with respect to the
# (real) inputs through the whole neural network. See dPropLayer for details
# It operates with real values only if realOnly = True
def dPropAll(Xin, CVNN, a, b, realOnly, actfun, Xmin = None, Xmax = None, cusp = None): # OX1, OX2
  if((Xmin is not None and Xmax is None) or (Xmin is None and Xmax is not None)):
    raise Exception('ERROR: Internal error. Optional arguments Xmin, Ymin must be both present or missing')
  # Sizes and size checks
  sizxin = Xin.shape
  nin = sizxin[0]
  min = sizxin[1]
  if(min != 1):
    raise Exception('ERROR: Invalid input vector. It must be a column vector')
  
  depthM1 = len(CVNN)
  # Depth of the perceptron counting the input layer
  depth = depthM1 + 1

  # CVNN cascade sizes check
  priorInputs = nin
  for l in range(depthM1):
    sizcvnn = CVNN[l].shape
    nw = sizcvnn[0]
    mw = sizcvnn[1]
    if(mw != priorInputs + 1):
      raise Exception('ERROR: Invalid weight matrix in cell CVNN (dimension mismatch)')
    priorInputs = nw
    

  # Feed forward
  ZC, OC = feedFwd(Xin, CVNN, a, b, realOnly, actfun)

  # Input level initialization for the 1st and 2nd derivatives
  OX1 = np.identity(nin)
  OX2 = np.zeros((nin, nin))

  # Layer by layer propagation
  for r in range(depthM1):
    # Propagation through hidden layer r
    OX1, OX2 = dPropLayer(nin, ZC, OC, OX1, OX2, CVNN, r, a, b, realOnly, actfun)

  # If the optional arguments Xmin, Xmax have been given, the gaussian envelope must be taken into account
  # First derivative = Output' * gaussian + Output .* dgaussian
  # Second derivative = Output'' * gaussian + Output' .* dgaussian + Output .* d2gaussian
  if(Xmin is not None and Xmax is not None and cusp is not None):
    OX2 = OX2 * gaussenv(Xin, Xmin, Xmax, cusp) + 2*OX1 * dgaussenv(Xin, Xmin, Xmax, cusp) + OC[depthM1-1] * d2gaussenv(Xin, Xmin, Xmax, cusp)
    OX1 = OX1 * gaussenv(Xin, Xmin, Xmax, cusp) + OC[depthM1-1] * dgaussenv(Xin, Xmin, Xmax, cusp)
    
  return OX1, OX2

###################################################################
# QVMC: QUANTUM VARIATIONAL MONTE CARLO
###################################################################

import numpy as np
from matplotlib import pyplot as plt

# FI f = FI(r, p) wavefunction
# Parameters passed are both system configuration 
# (dim(r) = 3 * no. of particles) and wave function parameters (ANN weights)
# in cell CVNN
def fi(r, CVNN, a, b, realOnly, actfun, Xmin = None, Xmax = None, cusp = None): # f
  # Dimension basic checks
  sizr = r.shape
  one1 = sizr[0]
  #print('one1' , one1)
  if(one1 != 1):
    raise Exception('ERROR: Invalid argument r: it must be a row vector with 3N components')

  ZC, OC = feedFwd(np.transpose(r), CVNN, a, b, realOnly, actfun, Xmin, Xmax, cusp)
  depthM1 = len(OC)
  sizout = OC[depthM1 - 1].shape  
  no_outputs = sizout[0]
                      
  if(no_outputs != 1):
    raise Exception('ERROR: Invalid neural network, only one neuron possible in output layer')
    
  f = OC[depthM1 - 1][0][0]

  return f


# NABLA2FI D2FI = NABLA2FI(r, p) nabla^2 operator on the wavefunction
# Parameters passed are both system configuration 
#(dim(r) = 3 * no. of particles) and wave function parameters in cell CVNN
def nabla2fi(r, CVNN, a, b, realOnly, actfun, Xmin = None, Xmax = None, cusp = None): # D2fi
  # Dimension basic checks
  sizr = r.shape
  one1 = sizr[0]

  #print(sizr) ###!!!

  if(one1 != 1):
    raise Exception('ERROR: Invalid argument r: it must be a row vector with 3N components')
  
  # Second derivative propagation evaluated at r, returned in vector OX
  # (one component per input to network)
  OX1, OX2 = dPropAll(np.transpose(r), CVNN, a, b, realOnly, actfun, Xmin, Xmax, cusp)
    
  sizOX2 = OX2.shape
  no_outputs = sizOX2[0]
  if(no_outputs != 1):
    raise Exception('ERROR: Invalid neural network, only one neuron possible in output layer')
    
  D2fi = np.sum(OX2)

  return D2fi


# HFI Ef = HFI(r, p) hamiltonian operator on the wavefunction
# Parameters passed are both system configuration 
# (dim(r) = 3 * no. of particles) and wave function parameters (ANN weights)
# in cell CVNN
# problem can be "hydra" for the hydrogen atom, "harmo" for the harmonic oscillator etc.
#fi(r, CVNN, a, b, realOnly, actfun, Xmin = None, Xmax = None):
#dPropAll(Xin, CVNN, a, b, realOnly, actfun, Xmin = None, Xmax = None)
def Hfi(r, CVNN, a, b, realOnly, actfun, problem, Xmin = None, Xmax = None, cusp = None): # Ef
  if(problem != "hydra" and problem != "harmo"):
    raise Exception('ERROR: Internal error. Potential for required problem unknown')
  sizr = r.shape
  one1 = sizr[0]

  if(one1 != 1):
    raise Exception('ERROR: Invalid argument r: it must be a row vector with 3N components')
  
  a0 = 1837/1836 # Bohr's radius in atomic units ?
  w = 1 # For the harmonic oscillator, mass = me = 1
  if(problem == "hydra"):
    Ef = -(a0/2) * nabla2fi(r, CVNN, a, b, realOnly, actfun, Xmin, Xmax, cusp) - fi(r, CVNN, a, b, realOnly, actfun, Xmin, Xmax, cusp) / np.linalg.norm(r, ord=2)
  elif(problem == "harmo"):
    Ef = -(1/2) * nabla2fi(r, CVNN, a, b, realOnly, actfun, Xmin, Xmax, cusp) + fi(r, CVNN, a, b, realOnly, actfun, Xmin, Xmax, cusp) * (1/2) * w**2 * np.linalg.norm(r, ord=2)**2
  return Ef


# ELOCAL EL = FI^-1(r, p)H FI(r, p)
# Parameters passed are both system configuration 
# (dim(r) = 3 * no. of particles) and ANN wave function weight parameters 
# in cell CVNN
# Returns the local energy and the wave function at r
def Elocal(r, CVNN, a, b, realOnly, actfun, problem, Xmin = None, Xmax = None, cusp = None): # EL, fir
  # Dimension basic checks
  sizr = r.shape
  one1 = sizr[0]
 
  if(one1 != 1):
    raise Exception('ERROR: Invalid argument r. It must be a row vector with 3N components')

  fir = fi(r, CVNN, a, b, realOnly, actfun, Xmin, Xmax, cusp)
  if(np.abs(fir) != 0):
    EL = Hfi(r, CVNN, a, b, realOnly, actfun, problem, Xmin, Xmax, cusp) / fir
  else:
    EL = 0

  return EL, fir  


# MARKOV generates a Markov Chain of M points for neural network
# parameters CVNN, starting at r0 and using random box moves of size box
# Function returns rf matrix with configuration vectors r in rows
def markov(M, CVNN, a, b, r0, box, realOnly, actfun, Xmin = None, Xmax = None, cusp = None): # rf, firf
  # Number of inputs retrieve
  sizCVNN0 = CVNN[0].shape
  ninp1 = sizCVNN0[1]
  nin = ninp1 - 1
  # Configuration vectors allocation
  rf = np.zeros((M, nin))
  # Wave function allocation
  firf = np.zeros((M, 1), dtype=complex) # Wave function
  #print('nin ', nin)
  nRejected = 0 # rejection counter
  rf[0, :] = r0 * np.ones((1, nin))
  # Wave function at the starting point
  firf[0] = fi(rf[0, :] * np.ones((1, nin)), CVNN, a, b, realOnly, actfun, Xmin, Xmax, cusp)  

  for f in range(1, M):
    # Next position is tried moving away randomly according to (-1 1) * box
    rf[f, :] = rf[f-1, :] + box * 2 * (np.random.rand(1, nin) - 0.5 * np.ones((1, nin)))
    # Calculation of the acceptance probability
    # fi(r, CVNN, a, b, realOnly, actfun, Xmin = None, Xmax = None)
    fi1 = fi(rf[f, :] * np.ones((1, nin)), CVNN, a, b, realOnly, actfun, Xmin, Xmax, cusp)
    firf[f] = fi1
    fi0 = fi(rf[f-1, :] * np.ones((1, nin)), CVNN, a, b, realOnly, actfun, Xmin, Xmax, cusp)
    accp = min(1.0, np.abs(fi1) ** 2 / np.abs(fi0) ** 2)
    # Roulette
    roulette = np.random.rand()
    if(roulette > accp): # Rollback and counter increment
      rf[f, :] = rf[f-1, :]
      firf[f] = fi0
      nRejected = nRejected + 1

  print("CVNNx, Markov chain % moves rejected:", 100 * nRejected / M);

  return rf, firf

# DOTPRODUCT approximates the scalar product of two wave functions <fi0|fi1>
# provided that a random walk is available (in rf) that has been sampled with fi0
def dotproduct(fi0, fi1):
  # Sizes check
  sizfi0 = fi0.shape
  sizfi1 = fi1.shape
  M = sizfi0[0]
  if(sizfi1[0] != M or sizfi0[1] != 1 or sizfi1[1] != 1):   
    raise Exception('ERROR: Inconsistent wavefunction sizes')

  dotp = np.sum(np.conj(fi1) / np.conj(fi0)) / M
  return dotp


# Evar calculates the variational energy given a Markov chain in matrix rf
# for ANN weight parameters in cell CVNN and ANN hyperparameters a and b
def Evar(rf, CVNN, a, b, realOnly, actfun, problem, Xmin = None, Xmax = None, cusp = None): # Ev, varEv
  # Size retrieval (number of samples in Markov chain)
  sizrf = rf.shape
  M = sizrf[0]
  nin = sizrf[1]
    
  # Local energy vector allocation
  Elc = np.zeros((M, 1), dtype=complex)
  # Local variance vector allocation
  Elv = np.zeros((M, 1), dtype=complex)
  # Local and variational energy
  for f in range(M):
    Elc[f][0], fir = Elocal(rf[f, :] * np.ones((1, nin)), CVNN, a, b, realOnly, actfun, problem, Xmin, Xmax, cusp)
  Ev = np.sum(Elc) / M
  # Variance of Ev
  for f in range(M):
    Elv[f][0] = (np.abs(Elc[f][0] - np.real(Ev)))**2
  varEv = np.sum(Elv) / M

  return Ev, varEv


# GRADEV gradient of the variational energy with respect to the weight parameters
def gradEv(rf, CVNN, a, b, realOnly, actfun, problem, Xmin = None, Xmax = None, cusp = None): # GCVNN, Ev, varEv, firf
  # Size retrieval (number of samples in Markov chain)
  sizrf = rf.shape
  M = sizrf[0]
  nin = sizrf[1]
  # Gradient lists of arrays allocation
  GCVNN = klistarray(0, CVNN)
  AUX1G = GCVNN
  #AUX2G = GCVNN
  # Local energy and wave function vector allocation
  Elc = np.zeros((M, 1), dtype=complex) # Local energy
  firf = np.zeros((M, 1), dtype=complex) # Wave function
  Elv = np.zeros((M, 1), dtype=complex) # Temporary vector for variance calculation
  for f in range(M):    
    # Local energy
    Elc[f], firf[f] = Elocal(rf[f, :] * np.ones((1, nin)), CVNN, a, b, realOnly, actfun, problem, Xmin, Xmax, cusp)
  # Variational energy (it is real +- imaginary error)
  Ev = np.sum(Elc) / M
  EvR = np.real(Ev) # EvI has to be small

  # Variance of Ev
  for f in range(M):
    Elv[f][0] = (np.abs(Elc[f][0] - EvR))**2
  varEv = np.sum(Elv) / M
  
  for f in range(M):
    # Gradients of the wave function with respect to the real and imaginary parts
    gradReFi = feedBwdGradient(np.transpose(rf[f,:] * np.ones((1, nin))), (0+0j) * np.ones((1, 1)), CVNN, a, b, realOnly, actfun, "gradfiR", Xmin, Xmax, cusp)
    gradImFi = feedBwdGradient(np.transpose(rf[f,:] * np.ones((1, nin))), (0+0j) * np.ones((1, 1)), CVNN, a, b, realOnly, actfun, "gradfiI", Xmin, Xmax, cusp)
    # Wave function and its complex-conjugate
    wf = firf[f]
    wfR = np.real(wf)
    wfI = np.imag(wf)
    wfabs2 = wfR**2 + wfI**2
    #wfc = np.conj(wf)
    
    ElcR = np.real(Elc[f])
    ElcI = np.imag(Elc[f])
    if(wfabs2 != 0): # Contribute to integral only if the wave function is not zero
      #AUX1G = sumlistarray(AUX1G, sumlistarray(klistarray((2/wfabs2)*(wfR * (ElcR - EvR) - wfI * ElcI), gradReFi), klistarray((2/wfabs2)*(wfI * (ElcR - EvR) - wfR * ElcI), gradImFi)))
      AUX1G = sumlistarray(AUX1G, sumlistarray(klistarray((2/wfabs2)*(wfR * (ElcR - EvR) - wfI * ElcI), gradReFi), klistarray((2/wfabs2)*(wfI * (ElcR - EvR) + wfR * ElcI), gradImFi))) 
  # Integrated gradients
  GCVNN = klistarray(1 / M, AUX1G)

  return GCVNN, Ev, varEv, firf

# Markov gradient descent
def markGradDesc(CVNN0, M, r0, box, alfa, beta1, beta2, epsilon, Neps, a, b, realOnly, factlist, qproblem, Xmin, Xmax, cusp = None, fileroot = None): # CVNN, E0, var0
  # TO BE COMPLETED: Checks
  np.random.seed(123)
  datafile = fileroot + ".txt"
  fileo = open(datafile,"w")
  fileo.write("Epochs RealEv ImagEv Variance Grad\r\n")
  fileo.close()

  # Trial vector allocation
  CVNNEP = klistarray(1, CVNN0)
  t = 0 # Counter for Adam
  mt = klistarray(0, CVNN0) # Auxiliary vectors for Adam
  vt = klistarray(0, CVNN0)
  depthM1 = len(CVNNEP)
  countWoMarkov = 0 # New random walk counter

  for ep in range(Neps): # Iterate in ep (epochs)
    rf, fi0 = markov(M, CVNNEP, a, b, r0, box, realOnly, factlist, Xmin, Xmax, cusp)
    countWoMarkov = 0
    tR = np.zeros((M, 1))
    # Theoretical solution to be plotted
    theoSol = np.zeros((M, 1), dtype="complex")
    for im in range(M):
      tR[im][0] = np.linalg.norm(rf[im][0:3], ord=2)
      theoSol[im][0] = (-1+1j) * np.exp(-tR[im][0])
    #plt.title("Real") # 2DHO
    #plt.xlabel("r (input)") 
    #plt.scatter(np.real(tR), np.real(fi0))
    #plt.show()
    #plt.title("Imaginary") 
    #plt.xlabel("r (input)") 
    #plt.scatter(np.real(tR), np.imag(fi0))
    #plt.show()
    fig, axs = plt.subplots(1, 1)
    fig.suptitle('Hydrogen Atom')
    axs.set_xlim([0, 5])
    axs.set(xlabel = "r", ylabel="Real(fi)")
    axs.scatter(np.real(tR), np.real(fi0) / np.max(np.abs(np.real(fi0))), c = "tab:orange")
    axs.scatter(np.real(tR), np.real(theoSol), c = "tab:green")
    fig.savefig(fileroot + "re" + str(ep) + ".png")
    plt.close(fig)
    
    fig, axs = plt.subplots(1, 1)
    fig.suptitle('Hydrogen Atom')
    axs.set_xlim([0, 5])
    axs.set(xlabel = "r", ylabel="Imag(fi)")
    axs.scatter(np.real(tR), np.imag(fi0) / np.max(np.abs(np.imag(fi0))), c = "tab:orange")
    axs.scatter(np.real(tR), np.imag(theoSol), c = "tab:green")
    fig.savefig(fileroot + "im" + str(ep) + ".png")
    plt.close(fig)
    

    dotp = 1
    modgrad2 = 100 # Chapucilla

    # 4 mini-batches of Markov's chain
    GCVNN, Ev, varEv, firf = gradEv(rf, CVNNEP, a, b, realOnly, factlist, qproblem, Xmin, Xmax, cusp)
    modgrad2 = modcvnn2(GCVNN)

    print("Ev  ", Ev)
    print("var ", varEv)
    print("grd ", np.sqrt(modgrad2))
    fileo = open(datafile,"a+")
    fileo.write("%d %f %f %f %f\r\n" % (ep, np.real(Ev), np.imag(Ev), np.real(varEv), np.sqrt(modgrad2)))
    fileo.close()

    # Adam #
    t = t + 1 
    mt = sumlistarray(klistarray(beta1, mt), klistarray(1 - beta1, GCVNN))
    vt = sumlistarray(klistarray(beta2, vt), klistarray(1 - beta2, multlistarray(GCVNN, GCVNN)))
    stepW = divlistarray(klistarray(-alfa / (1 - np.power(beta1, t)), mt), sqrtlistarray(klistarray(1 / (1 - np.power(beta2, t)), vt)), epsilon)
    CVNNM1 = klistarray(1, CVNNEP)
    CVNNEP = sumlistarray(CVNNEP, stepW)
  
  #fileo.close()
  return CVNNM1, Ev, varEv

# QUANTUM PROBLEM2 : Two-dimensional harmonic oscillator
import sys
import time

# Parameters
qproblem = "hydra"
cusp = True
a = np.array([[0.695],[0.695],[0.695]]) # asinh, asinh, asinh
b = np.array([[1.75],[1.75],[1.75]]) 
factlist = ["asinh", "asinh", "asinh"]
realOnly = False

Xmin = np.array(([[-4],[-4],[-4]])) # 3DHO
Xmax = np.array(([[4],[4],[4]]))
box = 3 * 0.45 # Walker's jump 3DHO
M = 45000 # configuration points 3D
r0 = np.array(([[0.1,0.1,0.1]])) # 3DHO

# Adam hyperparameters
alfa = 0.005
beta1 = 0.900
beta2 = 0.999
epsilon = 1e-8

# Weights initialization
# Supervised training dataset generation (complex)
# Calculation of an ANN that approximates the (complex) gaussian envelope * (1 + i)
# Objective: to study complex and real CVNN architectures that approximates the series
# Returns the input signal batch tXin, the complex signal tDout, and the norm of the input (batch) for scatter plots
def dataSetC(xmin, xmax, xcuts, ymin, ymax, ycuts, zmin, zmax, zcuts): # tXin, tDout, tR
  # Dataset size
  sizm = xcuts * ycuts * zcuts

  # Matrix allocation
  tXin = np.zeros((sizm, 3), dtype=complex)
  tDout = np.zeros((sizm, 1), dtype=complex)
  tR = np.zeros((sizm, 1))
  row = 0
  for x in np.linspace(xmin, xmax, xcuts):
    for y in np.linspace(ymin, ymax, ycuts):
      for z in np.linspace(zmin, zmax, zcuts):
        tXin[row][0:3] = np.array(([[x, y, z]]))
        # Normalized gaussian response with equal real and imaginary parts
        tDout[row][0] = (-1 + 1j)*gaussenv(np.array(([[tXin[row][0]],[tXin[row][1]],[tXin[row][2]]])), np.array(([[xmin],[ymin],[zmin]]), dtype = complex), np.array(([[xmax],[ymax],[zmax]]), dtype = complex), cusp) # TESTED GAUSSENV 3D COMPLEX
        tR[row][0] = np.linalg.norm(tXin[row][0:3], ord=2)
        row = row + 1
 
  return tXin, tDout, tR

commonseed = 0  # Check if it was 0 in tested files
nepochsst = 100 # HO 1D Complex and Real Unfair
#nepochsst = 150
batch = 500
# Network initialization for the gaussian envelope
s = time.time()
#for nfou in range(0, 4):

print('ITERATIONS -------------------------------- >>>>>>>>>>>> 500')
# COMPLEX CASE
np.random.seed(commonseed)
xmin = -5 # 2D /3D HO
xmax = 5

# iniNetwork(dimLine, realOnly, bias0, fouser)
CVNN = iniNetwork(np.array([[3, 32, 8, 4, 1]]), realOnly, False, False, Xmin, Xmax)
tXin, tDout, tR = dataSetC(xmin, xmax, 30, xmin, xmax, 30, xmin, xmax, 30) # Montecarlo 3DHO 27000 points
                               
# Backpropagate
CVNN, e, testcurve = feedBwdAll(tXin, tDout, CVNN, a, b, realOnly, factlist, "bpstd", alfa, beta1, beta2, epsilon, batch, nepochsst, Xmin, Xmax, cusp, None)

print('Warning generated by plt.plot. Ignore')
plt.title("Real") 
plt.xlabel("r (input)") 
plt.scatter(np.transpose(np.real(tR)), np.transpose(np.real(tDout)))
plt.scatter(np.transpose(np.real(tR)), np.transpose(np.real(testcurve)))
#plt.show()
plt.savefig("genvre.png")
plt.close()
plt.title("Imaginary") 
plt.xlabel("r (input)") 
plt.scatter(np.transpose(np.real(tR)), np.transpose(np.imag(tDout)))
plt.scatter(np.transpose(np.real(tR)), np.transpose(np.imag(testcurve)))
#plt.show()
plt.savefig("genvim.png")
plt.close()

print('Total e:', e) # Complex case


elapse = time.time()
print(elapse - s)


old_stdout = sys.stdout
log_file = open("message.log","w")
sys.stdout = log_file

CVNNF, E0, var0 = markGradDesc(CVNN, M, r0, box, alfa, beta1, beta2, epsilon, 500, a, b, realOnly, factlist, qproblem, Xmin, Xmax, cusp, fileroot = "test2D")

sys.stdout = old_stdout
log_file.close()
